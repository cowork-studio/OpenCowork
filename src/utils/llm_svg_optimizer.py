#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Copyright (c) 2025 OfficeCowork Research Group.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

å¤§æ¨¡å‹é©±åŠ¨çš„SVGä¼˜åŒ–å™¨
ä½¿ç”¨Anthropic Claudeæˆ–OpenAI GPTé‡æ–°ç”Ÿæˆä¼˜åŒ–çš„SVGä»£ç 
"""

import json
import os
import re
import requests
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import xml.etree.ElementTree as ET

from advanced_svg_optimizer import AdvancedSVGOptimizer, OptimizationLevel, OptimizationReport


class LLMProvider(Enum):
    """æ”¯æŒçš„å¤§æ¨¡å‹æä¾›å•†"""
    ANTHROPIC = "anthropic"
    OPENAI = "openai"


@dataclass
class LLMConfig:
    """å¤§æ¨¡å‹é…ç½®"""
    provider: LLMProvider
    api_key: str
    model: str = None
    base_url: str = None
    
    def __post_init__(self):
        if self.model is None:
            if self.provider == LLMProvider.ANTHROPIC:
                self.model = "claude-3-sonnet-20240229"
            else:  # OpenAI
                self.model = "gpt-4"
        
        if self.base_url is None:
            if self.provider == LLMProvider.ANTHROPIC:
                self.base_url = "https://api.anthropic.com/v1/messages"
            else:  # OpenAI
                self.base_url = "https://api.openai.com/v1/chat/completions"


class LLMSVGOptimizer:
    """å¤§æ¨¡å‹é©±åŠ¨çš„SVGä¼˜åŒ–å™¨"""
    
    def __init__(self, llm_config: LLMConfig):
        self.llm_config = llm_config
        self.detector = AdvancedSVGOptimizer(OptimizationLevel.STANDARD)
        
        # è‹±æ–‡æç¤ºè¯æ¨¡æ¿
        self.prompt_template = """
You are an expert SVG designer tasked with completely redesigning an SVG diagram. I will provide you with:
1. Original SVG code
2. List of detected layout issues

Your task is to create a COMPLETELY NEW SVG design that:
- Extracts ONLY the text content and logical relationships from the original
- IGNORES all original positioning, colors, shapes, and layout
- Redesigns the layout from scratch with better spacing and organization
- Creates a fresh, modern, and well-organized visual representation

## Original SVG Code (for text content extraction only):
```xml
{original_svg}
```

## Detected Layout Issues (to avoid):
{issues_list}

## Design Requirements:
1. **Complete Layout Redesign**: Do NOT copy any positioning from the original
2. **Extract Text Only**: Identify all text elements and their logical relationships
3. **New Visual Hierarchy**: Create proper spacing, grouping, and flow
4. **Modern Design**: Use clean layouts, proper margins, and visual balance
5. **Prevent Issues**: Ensure no overlaps, proper text sizing, adequate spacing
6. **Maintain Logic**: Keep the conceptual relationships between elements clear
7. **Fresh Colors**: Use a modern, harmonious color palette
8. **Better Typography**: Improve text sizing and positioning for readability

## Analysis Steps:
1. Extract all text content from the original SVG
2. Identify the logical structure and relationships
3. Design a completely new layout that better represents these relationships
4. Choose new positions, colors, and arrangements
5. Ensure excellent spacing and no overlaps

## Response Format:
You MUST respond with a JSON object in this exact format:

```json
{{
  "optimized_svg": "<!-- Complete NEW SVG code here -->",
  "changes_made": [
    "Completely redesigned layout with improved spacing",
    "Reorganized elements for better visual hierarchy",
    "Applied modern color scheme",
    "Enhanced typography and text positioning"
  ],
  "issues_fixed": [
    "Eliminated all overlapping elements",
    "Improved text readability",
    "Created proper visual balance"
  ]
}}
```

The "optimized_svg" field must contain a completely redesigned SVG that looks significantly different from the original while preserving the core information and logical relationships.
"""

    def optimize_svg_with_llm(self, svg_content: str, output_path: Optional[str] = None) -> Tuple[str, Dict]:
        """
        ä½¿ç”¨å¤§æ¨¡å‹ä¼˜åŒ–SVG
        
        Args:
            svg_content: åŸå§‹SVGå†…å®¹
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Tuple[ä¼˜åŒ–åçš„SVGå†…å®¹, ä¼˜åŒ–æŠ¥å‘Š]
        """
        print("ğŸ” æ£€æµ‹SVGé—®é¢˜...")
        # ä½¿ç”¨ç°æœ‰æ£€æµ‹å™¨æ‰¾å‡ºé—®é¢˜
        issues = self.detector._detect_issues(svg_content)
        
        if not issues:
            print("âœ… æœªæ£€æµ‹åˆ°é—®é¢˜ï¼Œè¿”å›åŸå§‹SVG")
            return svg_content, {
                "issues_detected": [],
                "changes_made": [],
                "issues_fixed": [],
                "llm_used": False
            }
        
        print(f"ğŸ“‹ æ£€æµ‹åˆ° {len(issues)} ä¸ªé—®é¢˜")
        for i, issue in enumerate(issues[:5], 1):
            print(f"  {i}. {issue}")
        if len(issues) > 5:
            print(f"  ... è¿˜æœ‰ {len(issues)-5} ä¸ªé—®é¢˜")
        
        # æ„å»ºæç¤ºè¯
        issues_text = "\n".join([f"- {issue}" for issue in issues])
        prompt = self.prompt_template.format(
            original_svg=svg_content,
            issues_list=issues_text
        )
        
        print("ğŸ¤– å‘é€è¯·æ±‚åˆ°å¤§æ¨¡å‹...")
        # è°ƒç”¨å¤§æ¨¡å‹
        response = self._call_llm(prompt)
        
        # è§£æå“åº”
        optimized_svg, report = self._parse_llm_response(response, issues)
        
        # ä¿å­˜æ–‡ä»¶
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(optimized_svg)
            print(f"ğŸ’¾ ä¼˜åŒ–åçš„SVGå·²ä¿å­˜åˆ°: {output_path}")
        
        return optimized_svg, report
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨å¤§æ¨¡å‹API"""
        if self.llm_config.provider == LLMProvider.ANTHROPIC:
            return self._call_anthropic(prompt)
        else:
            return self._call_openai(prompt)
    
    def _call_anthropic(self, prompt: str) -> str:
        """è°ƒç”¨Anthropic Claude API"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŸç”ŸAnthropic APIè¿˜æ˜¯OpenAIå…¼å®¹æ ¼å¼
        if 'anthropic.com' in self.llm_config.base_url:
            # åŸç”ŸAnthropic APIæ ¼å¼
            headers = {
                "Content-Type": "application/json",
                "x-api-key": self.llm_config.api_key,
                "anthropic-version": "2023-06-01"
            }
            
            data = {
                "model": self.llm_config.model,
                "max_tokens": 8000,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            }
            
            response = requests.post(
                self.llm_config.base_url,
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code != 200:
                raise Exception(f"Anthropic API error: {response.status_code} - {response.text}")
            
            result = response.json()
            return result["content"][0]["text"]
        else:
            # OpenAIå…¼å®¹æ ¼å¼ï¼ˆå¤§å¤šæ•°ä»£ç†å’Œç¬¬ä¸‰æ–¹APIï¼‰
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.llm_config.api_key}"
            }
            
            data = {
                "model": self.llm_config.model,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": 8000,
                "temperature": 0.3
            }
            
            response = requests.post(
                self.llm_config.base_url + "/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code != 200:
                raise Exception(f"API error: {response.status_code} - {response.text}")
            
            result = response.json()
            return result["choices"][0]["message"]["content"]
    
    def _call_openai(self, prompt: str) -> str:
        """è°ƒç”¨OpenAI GPT API"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.llm_config.api_key}"
        }
        
        data = {
            "model": self.llm_config.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": 8000,
            "temperature": 0.3
        }
        
        response = requests.post(
            self.llm_config.base_url + "/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        
        if response.status_code != 200:
            raise Exception(f"OpenAI API error: {response.status_code} - {response.text}")
        
        result = response.json()
        return result["choices"][0]["message"]["content"]
    
    def _parse_llm_response(self, response: str, original_issues: List[str]) -> Tuple[str, Dict]:
        """è§£æå¤§æ¨¡å‹å“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”ä½œä¸ºJSON
                json_str = response.strip()
            
            data = json.loads(json_str)
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            if "optimized_svg" not in data:
                raise ValueError("å“åº”ä¸­ç¼ºå°‘ 'optimized_svg' å­—æ®µ")
            
            optimized_svg = data["optimized_svg"]
            
            # éªŒè¯SVGæ ¼å¼
            if not optimized_svg.strip().startswith("<svg"):
                # å¦‚æœä¸æ˜¯ä»¥<svgå¼€å¤´ï¼Œå°è¯•æå–SVGéƒ¨åˆ†
                svg_match = re.search(r'(<svg.*?</svg>)', optimized_svg, re.DOTALL)
                if svg_match:
                    optimized_svg = svg_match.group(1)
                else:
                    raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„SVGä»£ç ")
            
            # éªŒè¯SVGæ˜¯å¦æ˜¯æœ‰æ•ˆçš„XML
            try:
                ET.fromstring(optimized_svg)
            except ET.ParseError as e:
                raise ValueError(f"ç”Ÿæˆçš„SVGä¸æ˜¯æœ‰æ•ˆçš„XML: {e}")
            
            report = {
                "issues_detected": original_issues,
                "changes_made": data.get("changes_made", []),
                "issues_fixed": data.get("issues_fixed", []),
                "llm_used": True,
                "llm_provider": self.llm_config.provider.value,
                "llm_model": self.llm_config.model
            }
            
            return optimized_svg, report
            
        except json.JSONDecodeError as e:
            raise Exception(f"æ— æ³•è§£æå¤§æ¨¡å‹å“åº”ä¸ºJSON: {e}\n\nå“åº”å†…å®¹:\n{response}")
        except Exception as e:
            raise Exception(f"è§£æå¤§æ¨¡å‹å“åº”æ—¶å‡ºé”™: {e}\n\nå“åº”å†…å®¹:\n{response}")
    
    def optimize_svg_file(self, input_file: str, output_file: str) -> Dict:
        """
        ä¼˜åŒ–SVGæ–‡ä»¶
        
        Args:
            input_file: è¾“å…¥SVGæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºSVGæ–‡ä»¶è·¯å¾„
            
        Returns:
            ä¼˜åŒ–æŠ¥å‘Š
        """
        if not os.path.exists(input_file):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        
        with open(input_file, 'r', encoding='utf-8') as f:
            svg_content = f.read()
        
        optimized_svg, report = self.optimize_svg_with_llm(svg_content, output_file)
        
        return report


def create_llm_optimizer_from_config() -> LLMSVGOptimizer:
    """ä»config/config.txté…ç½®æ–‡ä»¶åˆ›å»ºLLMä¼˜åŒ–å™¨"""
    # å¯¼å…¥config_loader
    import sys
    import os
    
    # æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    src_dir = os.path.join(current_dir, 'src')
    if src_dir not in sys.path:
        sys.path.append(src_dir)
    
    try:
        from config_loader import get_api_key, get_api_base, get_model
        
        # ä»config.txtè¯»å–é…ç½®
        api_key = get_api_key()
        api_base = get_api_base()
        model = get_model()
        
        if not api_key or api_key == 'your key':
            raise ValueError(
                "æœªé…ç½®æœ‰æ•ˆçš„APIå¯†é’¥ï¼Œè¯·åœ¨config/config.txtä¸­è®¾ç½®api_key\n"
                "æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ ANTHROPIC_API_KEY æˆ– OPENAI_API_KEY"
            )
        
        if not api_base:
            raise ValueError("æœªé…ç½®API base URLï¼Œè¯·åœ¨config/config.txtä¸­è®¾ç½®api_base")
        
        print(f"ğŸ”§ ä»config.txtè¯»å–é…ç½®:")
        print(f"  API Base: {api_base}")
        print(f"  Model: {model}")
        
        # æ ¹æ®API baseæˆ–modelåˆ¤æ–­ä½¿ç”¨å“ªä¸ªæä¾›å•†
        if api_base and model:
            api_base_lower = api_base.lower()
            model_lower = model.lower() if model else ""
            
            if 'anthropic' in api_base_lower or 'claude' in model_lower:
                provider = LLMProvider.ANTHROPIC
                print("ğŸ¤– è¯†åˆ«ä¸º Anthropic Claude API")
            elif 'openai' in api_base_lower or 'gpt' in model_lower:
                provider = LLMProvider.OPENAI  
                print("ğŸ¤– è¯†åˆ«ä¸º OpenAI GPT API")
            else:
                # é»˜è®¤å°è¯•OpenAIæ ¼å¼ï¼ˆå…¼å®¹æ›´å¤šAPIï¼‰
                provider = LLMProvider.OPENAI
                print("ğŸ¤– ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼")
        else:
            # å›é€€åˆ°ç¯å¢ƒå˜é‡æ£€æŸ¥
            anthropic_key = os.getenv('ANTHROPIC_API_KEY')
            openai_key = os.getenv('OPENAI_API_KEY')
            
            if anthropic_key:
                provider = LLMProvider.ANTHROPIC
                api_key = anthropic_key
                print("ğŸ”§ å›é€€åˆ°ç¯å¢ƒå˜é‡ ANTHROPIC_API_KEY")
            elif openai_key:
                provider = LLMProvider.OPENAI
                api_key = openai_key
                print("ğŸ”§ å›é€€åˆ°ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
            else:
                # é»˜è®¤ä½¿ç”¨OpenAIæ ¼å¼
                provider = LLMProvider.OPENAI
                print("ğŸ”§ ä½¿ç”¨é»˜è®¤ OpenAI æ ¼å¼")
        
        config = LLMConfig(
            provider=provider,
            api_key=api_key,
            model=model,
            base_url=api_base
        )
        
        return LLMSVGOptimizer(config)
        
    except ImportError as e:
        print(f"âš ï¸ æ— æ³•å¯¼å…¥config_loader: {e}")
        # å›é€€åˆ°ç¯å¢ƒå˜é‡
        return create_llm_optimizer_from_env()
    except Exception as e:
        print(f"âš ï¸ è¯»å–é…ç½®å¤±è´¥: {e}")
        # å›é€€åˆ°ç¯å¢ƒå˜é‡
        return create_llm_optimizer_from_env()


def create_llm_optimizer_from_env() -> LLMSVGOptimizer:
    """ä»ç¯å¢ƒå˜é‡åˆ›å»ºLLMä¼˜åŒ–å™¨ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
    # æ£€æŸ¥Anthropic API Key
    anthropic_key = os.getenv('ANTHROPIC_API_KEY')
    openai_key = os.getenv('OPENAI_API_KEY')
    
    if anthropic_key:
        config = LLMConfig(
            provider=LLMProvider.ANTHROPIC,
            api_key=anthropic_key
        )
        print("ğŸ”§ ä½¿ç”¨ Anthropic Claude (ç¯å¢ƒå˜é‡)")
    elif openai_key:
        config = LLMConfig(
            provider=LLMProvider.OPENAI,
            api_key=openai_key
        )
        print("ğŸ”§ ä½¿ç”¨ OpenAI GPT (ç¯å¢ƒå˜é‡)")
    else:
        raise ValueError(
            "è¯·åœ¨config/config.txtä¸­é…ç½®api_keyå’Œapi_baseï¼Œ\n"
            "æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ ANTHROPIC_API_KEY æˆ– OPENAI_API_KEY\n"
            "ä¾‹å¦‚: export ANTHROPIC_API_KEY='your-key-here'"
        )
    
    return LLMSVGOptimizer(config)


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python llm_svg_optimizer.py <input.svg> <output.svg>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2]
    
    try:
        optimizer = create_llm_optimizer_from_config()
        report = optimizer.optimize_svg_file(input_file, output_file)
        
        print("\n" + "="*50)
        print("ğŸ“Š ä¼˜åŒ–æŠ¥å‘Š")
        print("="*50)
        print(f"æ£€æµ‹åˆ°é—®é¢˜: {len(report['issues_detected'])}ä¸ª")
        print(f"ä½¿ç”¨çš„å¤§æ¨¡å‹: {report.get('llm_provider', 'unknown')} - {report.get('llm_model', 'unknown')}")
        
        if report['changes_made']:
            print("\nğŸ”§ ä¸»è¦ä¿®æ”¹:")
            for change in report['changes_made']:
                print(f"  â€¢ {change}")
        
        if report['issues_fixed']:
            print("\nâœ… ä¿®å¤çš„é—®é¢˜:")
            for fix in report['issues_fixed']:
                print(f"  â€¢ {fix}")
                
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)
